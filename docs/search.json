[
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Conference presentations that I have participated in are listed here."
  },
  {
    "objectID": "presentations.html#section",
    "href": "presentations.html#section",
    "title": "Presentations",
    "section": "2022",
    "text": "2022\nLaMontagne, J. M., Crone, E. E., Redmond, M., Barton, J., Bell, D., Chaudhary, V. B., Chen, A., Cleavitt, N., Greene, D., Holland, E. P., Johnstone, J., Koenig, W., Lyon, N., Macias, D., Miller, T., Nigro, K., Pearse, I. S., Satake, A., Schulze, M., Slette, I., Snell, R., & Zimmerman, J. (2022, September 20). Cross-site synthesis: Patterns & drivers of plant reproduction across LTER sites. LTER All Scientists‚Äô Meeting, Pacific Grove, CA, United States.\nLink: poster"
  },
  {
    "objectID": "presentations.html#section-1",
    "href": "presentations.html#section-1",
    "title": "Presentations",
    "section": "2021",
    "text": "2021\nBachelder, N. R., Chen, A., Zoe, F., Rapaport, M. K., Bang, J., Solomon, S. J., Lee, M. J., & Seltmann, K. C. (2021, August 5). Leveraging Large Biological Interaction Data to Quantify Plant Specialization by Bees. Ecological Society of America Meeting, Virtual. Retrieved from https://escholarship.org/uc/item/33b2t2bq\nLink: poster"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications üìñ",
    "section": "",
    "text": "Here are the publications I‚Äôve been a part of!\nMaurer, G. E. et al.¬†Ten simple rules for team synthesis in ecological research. [In prep]\nOhlert, T. et al.¬†(2025). Drought intensity and duration interact to magnify losses in primary productivity. Science, 390(6770), 284-289. doi.org/10.1126/science.ads8144.\nMalone, S. L., McLeod, G., Chen, A. & Tupaj, M. (2025). Contemporary Fire Regimes of the Subtropical Everglades. Sci Data, 12, Article 1622. doi.org/10.1038/s41597-025-05917-6.\nBrun, J. et al.¬†(2025). Enabling data-driven collaborative and reproducible environmental synthesis science. Methods in Ecology and Evolution, 16(6), 1061‚Äì1074. doi.org/10.1111/2041-210X.70036.\nNigro, K. M. et al.¬†(2025). Co-mast: Harmonized seed production data for woody plants across US long-term research sites. Ecology, 106(1), Article e4463. doi.org/10.1002/ecy.4463.\nLaMontagne, J. M. et al.¬†(2024). Community Synchrony in Seed Production is Associated With Trait Similarity and Climate Across North America. Ecology Letters, 27(12), Article e14498. doi.org/10.1111/ele.14498."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here are some projects I‚Äôve worked on as an undergrad in UC Santa Barbara!"
  },
  {
    "objectID": "projects.html#credit-card",
    "href": "projects.html#credit-card",
    "title": "Projects",
    "section": "Credit Card üí≥",
    "text": "Credit Card üí≥\nIn this project, I worked with my team to predict whether the client will default on credit card payments based on their gender, education, marital status, age, amount of the given credit, history of past payment, amount of bill statements and amount of previous payments.\nWe built the pipeline to preprocess the data and construct models. We applied several feature transformers such as VectorAssembler and StandardScaler from the PySpark ML package. The four models we developed were Logistic Regression, Decision Tree, Random Forest and Support Vector Machine. We evaluated our models‚Äô performance using the metrics derived from the confusion matrix, and then finally chose Support Vector Machine as our champion model because it has the best accuracy of 0.8065.\nLinks: project report and GitHub repo"
  },
  {
    "objectID": "projects.html#bees",
    "href": "projects.html#bees",
    "title": "Projects",
    "section": "Bees üêù",
    "text": "Bees üêù\nLarge, open-access biological datasets, like those hosted by Global Biotic Interactions (GloBI), have become increasingly accessible due to greater data collection, compilation, and storage. These databases serve to better inform our understanding of species occurrences, interactions, and ecosystem structure, broadly. In this work, we leverage GloBI data to better understand patterns of pollination, a biologically and economically essential biotic interaction between plants and pollinators. Specifically, we sought to develop a better understanding of bee specialization of pollen, an evolutionary trait in bees that underscores the stability and structure of pollinator interaction networks. We compared GloBI and expert-compiled data to better understand patterns in resource specialization. We then trained various machine learning models (Decision Trees, Support Vector Machine, Logistic Regression) to create a defining line between specialist and non-specialist bees. In addition, data transformation was also useful for more clearly differentiating our specialization groups.\nThrough our exploration of GloBI, we found several sources of bias, including the limitations of community data collection and scarcity of rare bees. We found a strong positive correlation between the number of sources (i.e.¬†literature, natural history collection) citing the interactions of a bee species and the number of plant families visited by that same bee species. We also found that while expert classification of bee specialists visit fewer plant families than other bees in the GloBI dataset, there are clusters of species that diverge from the expected trend. Our trained models suggested that binary classification was not completely effective in determining the label of ‚Äúspecialist‚Äù or ‚Äúnon-specialist‚Äù for all bee species. These findings indicate that observer bias, on a global scale, can skew our definition of resource specialization or generalization. Moreover, large, open-access datasets like GloBI can change our previous understanding of biological interactions and systems by accessing novel data sources and aggregation.\nLinks: project report and ESA 2021 meeting poster"
  },
  {
    "objectID": "projects.html#renewable-energy",
    "href": "projects.html#renewable-energy",
    "title": "Projects",
    "section": "Renewable Energy ‚òÄÔ∏è",
    "text": "Renewable Energy ‚òÄÔ∏è\nRenewable energy consumption has been increasing in the United States over the past 20 years as people sought alternatives to fossil fuels. Sources of renewable energy include hydroelectric power, geothermal, solar, wind, and biomass. In this project, I investigate the total monthly consumption of renewable energy in the United States from January 2001 to January 2020 and try to figure out a model that can be used to forecast how much renewable energy will be used in the future.\nIn order to achieve this goal, I used the Box-Jenkins methodology. This method involves data transformation, differencing, examining autocorrelation and partial autocorrelation functions, model parameter estimation, checking for stationarity and invertibility, and diagnostic checking. To summarize my results, I came up with three plausible models for the data, but ended up with two satisfactory models that passed all checking. I concluded that one model was better than the other because it had lower AICc, an information criterion, and used it to predict renewable energy consumption for February 2019 to January 2020. My predictions followed the actual data values well, meaning that my model can be used for future forecasting, and it also supports the conclusion that renewable energy consumption is on an upward trend.\nLinks: project report and GitHub repo"
  },
  {
    "objectID": "projects.html#spam-emails",
    "href": "projects.html#spam-emails",
    "title": "Projects",
    "section": "Spam Emails üìß",
    "text": "Spam Emails üìß\nCertain emails are automatically considered spam by algorithms in email systems. These algorithms track patterns of certain keywords, which allows them to classify spam and non-spam emails. This dataset, spambase.data, allows us to explore the connection between the frequency of keywords and whether an email is classified as spam. If such a connection exists, it would be more convenient for email users because their spam emails will be filtered out before reaching them.\nMy team‚Äôs question: Is there a relationship between the predictors (frequency of keywords/characters, length of sequences, number of capital letters) and whether an email is considered spam or not? If so, which predictors affect the response?\nLinks: project report and GitHub repo"
  },
  {
    "objectID": "projects.html#fossil-fuel",
    "href": "projects.html#fossil-fuel",
    "title": "Projects",
    "section": "Fossil Fuel ü¶ñ",
    "text": "Fossil Fuel ü¶ñ\nAs part of the UCSB Data Science Fellowship Tutoring Committee under Professor Yekaterina Kharitonova‚Äôs guidance, I created a lab project for undergrads in her Fall 2020 Introduction to Data Science 1 course. This lab project was made for the undergrads to practice their data analysis skills in Python. They were instructed to investigate how the COVID-19 lockdown affected monthly fossil fuel consumption in the United States from January 2001 to July 2020.\nLink: GitHub repo"
  },
  {
    "objectID": "selected.html",
    "href": "selected.html",
    "title": "Selected Code üëæ",
    "section": "",
    "text": "Below are some examples of the work I‚Äôve done for the Malone Disturbance Ecology Lab and the Long Term Ecological Research (LTER) Network synthesis working groups. I got to work on a variety of tasks, which allows me to broaden my flexible skill set."
  },
  {
    "objectID": "selected.html#data-wrangling",
    "href": "selected.html#data-wrangling",
    "title": "Selected Code üëæ",
    "section": "Data Wrangling üõ†",
    "text": "Data Wrangling üõ†\n\nMarine Consumer Nutrient Dynamics\nThe Marine Consumer Nutrient Dynamics working group had marine consumer datasets spanning across 8 sites, all varying in format and column names. After much discussion with the group, we designed a workflow where we created a data key to connect the raw column names to the new, standardized column names. Then I worked on a script where I reshaped the raw files and joined them with the data key to automate the column name standardization process for 12 datasets with completely different structures. I also standardized all dates, species names, and filled in missing taxonomic information for each species. This workflow resulted in a harmonized/standardized CSV file containing marine consumer information for over 700 unique species.\nLinks: GitHub repo\n\n\nPlant Reproduction\nThe Plant Reproduction working group wanted plant trait data to research why some plants produce large amounts of seeds every few years. To assist this group, my team and I retrieved data from the TRY Plant Trait Database, which included traits such as plant seed mass, lifespan, flowering season, and seed persistence. Then we wrangled the TRY data so that we can combine it with a larger dataset of plant traits that the working group already had. After much wrangling, we refined the integrated master dataset to include 56 total variables for over 100 species. This dataset was then used for further downstream analyses.\nLinks: GitHub repo"
  },
  {
    "objectID": "selected.html#exploratory-graphing",
    "href": "selected.html#exploratory-graphing",
    "title": "Selected Code üëæ",
    "section": "Exploratory Graphing üìà",
    "text": "Exploratory Graphing üìà\n\nPlant Reproduction\nThe Plant Reproduction working group requested me to help them explore a potential analysis for one of their manuscripts. They were interested in a time series plot showing the total seed mass production in grams per year at specific plots at a site. I manipulated the columns in the aforementioned integrated master dataset to calculate the grams of seed per species per year. Then I graphed the time series, with a separate panel for each plot.\nLinks: GitHub repo"
  },
  {
    "objectID": "selected.html#spatial-data",
    "href": "selected.html#spatial-data",
    "title": "Selected Code üëæ",
    "section": "Spatial Data üåé",
    "text": "Spatial Data üåé\n\nSilica Export\nThe Silica Export working group wanted to investigate drivers of riverine silicon exports which included variables like surface air temperature, lithology, precipitation, evapotranspiration, elevation, and net primary production. They requested for my team and I to identify and this spatiotemporal information for the 228 watersheds that they were interested in. In order to accomplish this, we first searched online for the datasets that best suited our needs. Then we extracted the spatial data for each watershed using various R packages. Finally, we summarized the extracted values and exported them in a harmonized format that was easy to use for downstream analyses.\nLinks: GitHub repo"
  },
  {
    "objectID": "selected.html#text-mining",
    "href": "selected.html#text-mining",
    "title": "Selected Code üëæ",
    "section": "Text Mining üìë",
    "text": "Text Mining üìë\n\nEcosystem Transitions\nThe Ecosystem Transitions working group needed to review over 3000 papers in order to prepare a meta-analysis on ecosystem transitions. They split the reading assignments between their group members, but to speed the process along, they requested for me to find a way to quickly decide whether a paper is worth reading or not.\nSo I created a script that filters and ranks the abstracts and titles based on positive and negative keywords. The more positive keywords an abstract and its associated title have, the more likely it was for the group to include the full paper in their meta-analysis. On the other hand, if a paper‚Äôs abstract/title has more negative keywords, it means that the group will probably not be interested in this paper.\nAfter discussing with the group, we decided that it would be helpful for the group members to see how many negative keywords were in each paper‚Äôs abstract and title. So I added a column to each person‚Äôs reading assignment list that shows the count of negative keywords for each abstract and title. That way, they could prioritize reading the abstracts that have the least negative keywords and save time by discarding the ones that have the most negative keywords.\nIn the end, the group was able to use my work to decide on the 700 or so papers that will be included in round 2 of the meta-analysis.\nLinks: GitHub repo"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops üß∞",
    "section": "",
    "text": "As part of the Long Term Ecological Research Network Office (LNO), I helped create some workshops in order to support our synthesis working groups. My colleagues and I have hosted these workshops numerous times on an as-needed basis for these groups."
  },
  {
    "objectID": "workshops.html#collaborative-coding-with-github",
    "href": "workshops.html#collaborative-coding-with-github",
    "title": "Workshops üß∞",
    "section": "Collaborative Coding with GitHub",
    "text": "Collaborative Coding with GitHub\n\nIn synthesis science, collaboration on code products is often integral to the productivity of the group. However, learning to use the software and graphical user interfaces that support this kind of teamwork can be a significant hurdle for teams that are already experts in their subject areas. This workshop is aimed at helping participants gain an understanding of the fundamental purpose and functioning of ‚Äúversion control‚Äù systems‚Äìspecifically GitHub‚Äìto help teams more effectively code collaboratively.\nLink: workshop website"
  },
  {
    "objectID": "workshops.html#coding-in-the-tidyverse",
    "href": "workshops.html#coding-in-the-tidyverse",
    "title": "Workshops üß∞",
    "section": "Coding in the tidyverse",
    "text": "Coding in the tidyverse\n\nFor teams that code using the R programming language, the most familiar tools are often part of ‚Äúbase R‚Äù meaning that those functions and packages come pre-loaded when R is installed. Relatively recently the tidyverse has emerged as a comprehensive suite of packages that can complement base R or serve as an alternative for some tasks. This includes packages like dplyr and tidyr as well as the perhaps infamous pipe operator (%&gt;%) among many other tools. This workshop is aimed at helping participants use the tidyverse equivalents of fundamental data wrangling tasks that learners may be used to performing with base R.\nLink: workshop website"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Angel Chen",
    "section": "",
    "text": "Hi, I‚Äôm Angel Chen! Welcome to my personal website ‚ú®\n\n \n  \n   \n  \n    \n     Email\n  \n  \n    \n     CV\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github\n  \n\n\n\n\nExperience\n\nNov 2024 ‚Äì Present: Data Scientist | Malone Disturbance Ecology Lab\nFeb 2022 ‚Äì Oct 2024: Data Analyst | National Center for Ecological Analysis and Synthesis (NCEAS)\nOct 2019 ‚Äì Feb 2022: Data Curator | Arctic Data Center (ADC)\n\n\n\nEducation\n\n2021: Bachelor of Science in Statistics & Data Science | University of California, Santa Barbara\n\n\n\nInterests\n\nOpen science\nReproducible workflows\nBirds\nFashion"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About ‚ò∫Ô∏è",
    "section": "",
    "text": "Thanks for visiting my page! My name is Angel and my interest in data science, open science, and reproducible workflows started from my days as a student at the University of California, Santa Barbara! Feel free to meander on this website to explore some examples of my work.\nCurrently, I work as a data scientist at the Malone Disturbance Ecology Lab, focusing on scripting reproducible workflows to clean and analyze diverse data sources, as well as providing technical support to my lab members. I also develop web apps and R packages for the lab along the way!\nPreviously, I worked as a data analyst at the National Center for Ecological Analysis and Synthesis (NCEAS), supporting the Long Term Ecological Research (LTER) Network synthesis working groups. I helped the working groups develop reproducible workflows to wrangle, harmonize, analyze, and visualize ecological datasets. I also hosted workshops for them.\nBefore joining the LTER, I was a data curator for NCEAS Arctic Data Center. I created metadata records using Ecological Metadata Language (EML) and archived ecological datasets from research projects focused in the Arctic. Ensuring that the metadata is complete is a crucial part of making the data finable, accessible, interoperable, and reusable by others in the future.\nIn my spare time, I like to look at and admire birds. My favorite birds include rock doves and mourning doves. Here are some pictures I‚Äôve taken over the years:\n\n\n\nA pigeon flaps open its wings to balance itself on my hand, Golden Gate Park, San Francisco\n\n\n\n\n\nA pigeon grabs some millet from my hand, Golden Gate Park, San Francisco\n\n\n\n\n\nA crested duck looks closely at the ground, Golden Gate Park, San Francisco\n\n\n\n\n\nA female duck with its ducklings, Golden Gate Park, San Francisco\n\n\n\n\n\nA flock of pigeons gathers around a pile of rice, Chinatown, San Francisco\n\n\n\n\n\nA collared dove stops by my yard, San Francisco"
  },
  {
    "objectID": "selected.html#collaborative-reproducible-workflows",
    "href": "selected.html#collaborative-reproducible-workflows",
    "title": "Selected Code üëæ",
    "section": "Collaborative, Reproducible Workflows üîÅ",
    "text": "Collaborative, Reproducible Workflows üîÅ\n\nEverglades Fire History\nTo investigate fire history patterns in the Florida Everglades as part of the Malone Disturbance Ecology Lab, I scripted a workflow that took raw fire perimeter shapefiles and transformed them into a clean, harmonized shapefile. Then using that shapefile, I derived annual rasters showing the areas that were burned in a particular year. I prepared the metadata for these data products and published them on the Environmental Data Initiative Repository.\nNext, I created custom functions to perform some calculations. I calculated the total number of fires and the time since the last fire using the annual rasters. I also calculated statistics using the harmonized fire perimeter shapefile in order to get the annual total area burned, total wildfire area burned, total prescribed fire area burned, mean fire size, and area/perimeter. To allow others to easily explore this spatiotemporal information, I developed an interactive Shiny web app.\nFinally, my supervisor, Dr.¬†Sparkle Malone, compiled a vegetation shapefile for the Everglades, calculated more summary statistics from my results, and designed visualizations for our publication. After I polished the visualizations, we were able to publish our findings in Scientific Data.\nLinks: GitHub repo, data products, Shiny web app, publication\n\n\nFlux Gradient\nThe Flux Gradient working group had an existing pipeline to download and analyze eddy covariance data for 8 NEON sites, however they requested me to rerun and expand their workflow to include an additional 39 more sites. After I downloaded the latest version of the data, I ran into errors, so I had to debug and edit the scripts accordingly, as there were some peculiarities with a few sites.\nMy debugging later led me to the discovery that the format of the data had changed, which was unsuitable for some parts of the workflow. I communicated this issue to the other members of the group in our meeting and they advised me to redownload a larger version of the dataset to get the necessary columns.\nDespite the small setback, I finished debugging and running the workflow for all 47 sites just in time for the working group‚Äôs in-person meeting. Since I did not have the domain knowledge, I had to explain to the group about the issues I ran into whenever I needed them to make a decision on the workflow for me. The teamwork enabled me to seamlessly process around 1 terabyte of data.\nAs our project evolved, our GitHub repository became bloated with everyone‚Äôs scripts so to ensure that our work was organized and reproducible, I standardized naming conventions for the main scripts and functions, fixed file paths, formatted scripts for publication, and revamped the README.\nLinks: GitHub repo"
  }
]